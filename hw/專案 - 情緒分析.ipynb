{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:51:45.097900Z",
     "start_time": "2021-04-28T11:51:20.507455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 10950\n",
      "Train accuracy: 0.783157894736842\n",
      "Test accuracy: 0.72\n",
      "unit -0.7325409911126661\n",
      "bad -0.7079166875260443\n",
      "cable 0.6821719552990964\n",
      "time -0.6998176501234197\n",
      "'ve 0.7742410035472721\n",
      "month -0.6687013020845857\n",
      "sound 1.0822704376449217\n",
      "lot 0.8264244970303497\n",
      "you 0.9307459602491117\n",
      "n't -2.111080608425142\n",
      "easy 1.729649358645354\n",
      "quality 1.329125891447414\n",
      "company -0.5602488518608839\n",
      "card -0.6088667937906855\n",
      "item -0.979212113036486\n",
      "wa -1.4880481682341744\n",
      "perfect 0.969446259726007\n",
      "fast 0.9926448950000607\n",
      "ha 0.6571586938848277\n",
      "price 2.802432765473389\n",
      "value 0.5522359577351388\n",
      "money -1.0105369092266132\n",
      "memory 0.9649587794947415\n",
      "buy -0.8724835584810097\n",
      "bit 0.6085916175146011\n",
      "happy 0.5548016332264535\n",
      "pretty 0.5566216504044649\n",
      "doe -1.238831332580061\n",
      "highly 1.0751342531053283\n",
      "recommend 0.6514289892205909\n",
      "customer -0.6625763582079012\n",
      "support -0.8643565295751044\n",
      "little 0.9296661361946765\n",
      "returned -0.8218108248759785\n",
      "excellent 1.3176802253957047\n",
      "love 1.1957539252117\n",
      "feature 0.5026506909976957\n",
      "home 0.5422681371251171\n",
      "piece -0.5091385374546683\n",
      "week -0.6738620032024564\n",
      "using 0.6255771282942418\n",
      "laptop 0.5755269131609216\n",
      "video 0.5437720174992999\n",
      "poor -0.77624279336521\n",
      "look 0.5013338479611442\n",
      "then -1.0548698069474742\n",
      "called -0.5240035434568393\n",
      "tried -0.7874820730635522\n",
      "static -0.5216634028048212\n",
      "try -0.6861839550905626\n",
      "usb 0.5212155529462107\n",
      "space 0.5991080159560501\n",
      "comfortable 0.6024713217077937\n",
      "hour -0.5814677341985158\n",
      "expected 0.5797665679618834\n",
      "speaker 0.9965140654000743\n",
      "warranty -0.5569092817860328\n",
      "stopped -0.5133015974417943\n",
      "junk -0.5432722116735719\n",
      "returning -0.5040275889804721\n",
      "paper 0.5081629414311929\n",
      "terrible -0.5110492959682579\n",
      "return -1.2190924640497445\n",
      "waste -1.0001027160654954\n",
      "refund -0.6084630143860905\n",
      "Most wrong positive review (prob = 0.35173759768737367, pred = 0.0):\n",
      "\n",
      "A device like this either works or it doesn't.  This one happens to work\n",
      "\n",
      "Most wrong negative review (prob = 0.6027947678021454, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 題目:電商產品評分文件以機器學習方式分辨是否為正向或負向\n",
    "#\n",
    "# 說明：輸入文件 positive.review 和 negative.review，兩者都是XML檔。我們用BeautifulSoup讀進來，\n",
    "# 擷取review_text，然後用NLTK自建Tokenizer。 先產生 word-to-index map 再產生 word-frequency vectors。\n",
    "# 之後 shuffle data 創造 train/test splits，留100個給 test 用。接著用Logistic Regression 分類器\n",
    "# 找出訓練組和測試組的準確度(Accuracy)。接著我們可以看看每個單字的正負權重，可以訂一個閥值，\n",
    "# 比方絕對值大於正負0.5，以確認情緒是顯著的。最後我們找出根據現有演算法歸類錯誤最嚴重的正向情緒和負向\n",
    "# 情緒的例子。\n",
    "#\n",
    "# 延伸:可用不同的tokenizer，不同的tokens_to_vector，不同的ML分類器做改進準確率的比較。最後可用您的\n",
    "# model去預測unlabeled.review檔的內容。\n",
    "#\n",
    "# 範例程式檔名: sentiment_情緒分析.py，以LogisticRegression 方式完成情緒分析。\n",
    "# 模組: sklearn, bs4, numpy, nltk\n",
    "# 輸入檔：stopwords.txt, /electronics 下 positive.review, negative.review\n",
    "# 成績：辨識百分率\n",
    "#\n",
    "#注意事項：nltk 需要有 punkt corpus 和 wordnet  資源\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet') \n",
    "#資料檔需在適當位置 jupyter 或 colab 才能看到，用colab時要上傳 data 到 ./sample_data 或 mount\n",
    "#\n",
    "#\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('./datasets/stopwords.txt'))\n",
    "\n",
    "# 另一個 stopwords 的來源\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')\n",
    "\n",
    "# 讀正向與負向 reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('./datasets/electronics/positive.review', encoding='utf-8').read(), features=\"html5lib\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('./datasets/electronics/negative.review', encoding='utf-8').read(), features=\"html5lib\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "\n",
    "\n",
    "# 基於nltk自建 tokenizer\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # 將字串改為tokens\n",
    "    tokens = [t for t in tokens if len(t) > 2] # 去除短字\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # 去除大小寫\n",
    "    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 先產生 word-to-index map 再產生 word-frequency vectors\n",
    "# 同時儲存 tokenized 版本未來不需再做 tokenization\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "orig_reviews = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(\"len(word_index_map):\", len(word_index_map))\n",
    "\n",
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # 最後一個元素是標記\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # 正規化數據提升未來準確度\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1) 矩陣 - 擺在一塊將來便於shuffle\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "# shuffle data 創造 train/test splits\n",
    "# 多次嘗試!\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# 最後 100 列是測試用\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))\n",
    "\n",
    "\n",
    "# 列出每個字的正負 weight\n",
    "# 用不同的 threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)\n",
    "\n",
    "\n",
    "# 找出歸類錯誤的例子\n",
    "preds = model.predict(X)\n",
    "P = model.predict_proba(X)[:,1] # p(y = 1 | x)\n",
    "\n",
    "# 只列出最糟的\n",
    "minP_whenYis1 = 1\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Y[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = orig_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = orig_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p\n",
    "\n",
    "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:00:47.179960Z",
     "start_time": "2021-04-28T12:00:47.172473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 2000, 2000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_reviews), len(negative_reviews), len(X), len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 專案流程\n",
    "- 資料取得與清洗\n",
    "- 文字轉向量\n",
    "- 包裝成train/test\n",
    "- 模型套用\n",
    "- 觀察錯誤樣本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:07:23.927092Z",
     "start_time": "2021-04-28T12:07:21.564570Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:09:03.703563Z",
     "start_time": "2021-04-28T12:09:03.540063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nI purchased this unit due to frequent blacko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nI ordered 3 APC Back-UPS ES 500s on the reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nWish the unit had a separate online/offline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nCheaper than thick CD cases and less prone t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nHi\\n\\nI brought 256 MB Kingston SD card from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  \\nI purchased this unit due to frequent blacko...\n",
       "1  \\nI ordered 3 APC Back-UPS ES 500s on the reco...\n",
       "2  \\nWish the unit had a separate online/offline ...\n",
       "3  \\nCheaper than thick CD cases and less prone t...\n",
       "4  \\nHi\\n\\nI brought 256 MB Kingston SD card from..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = positive_reviews + negative_reviews\n",
    "df = pd.DataFrame(df, columns=['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:09:27.928346Z",
     "start_time": "2021-04-28T12:09:27.905167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nI purchased this unit due to frequent blacko...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nI ordered 3 APC Back-UPS ES 500s on the reco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nWish the unit had a separate online/offline ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nCheaper than thick CD cases and less prone t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nHi\\n\\nI brought 256 MB Kingston SD card from...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  \\nI purchased this unit due to frequent blacko...      1\n",
       "1  \\nI ordered 3 APC Back-UPS ES 500s on the reco...      1\n",
       "2  \\nWish the unit had a separate online/offline ...      1\n",
       "3  \\nCheaper than thick CD cases and less prone t...      1\n",
       "4  \\nHi\\n\\nI brought 256 MB Kingston SD card from...      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:10:36.763464Z",
     "start_time": "2021-04-28T12:10:36.755198Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[1000:, 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:13:40.456776Z",
     "start_time": "2021-04-28T12:13:40.434051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>\\nTo back up my personal DVD collection I burn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>\\nWorks well....Customer support from Garmin.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>\\nMy husband bought me this for Christmas beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>\\nFor some years I tried this and other Monste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>\\nThe is perhaps the lease durable item I have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>\\nThis is a complete waste of money for a numb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>\\nI've read the other reviews of this product ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>\\nI would highly recommend reading the Busines...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>\\nThe product not only did not work but blew o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>\\nI'm the consumer of SanDisk USB flash drives...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "1047  \\nTo back up my personal DVD collection I burn...      0\n",
       "276   \\nWorks well....Customer support from Garmin.....      1\n",
       "1284  \\nMy husband bought me this for Christmas beca...      0\n",
       "1225  \\nFor some years I tried this and other Monste...      0\n",
       "1038  \\nThe is perhaps the lease durable item I have...      0\n",
       "...                                                 ...    ...\n",
       "1074  \\nThis is a complete waste of money for a numb...      0\n",
       "790   \\nI've read the other reviews of this product ...      1\n",
       "855   \\nI would highly recommend reading the Busines...      1\n",
       "1107  \\nThe product not only did not work but blew o...      0\n",
       "1045  \\nI'm the consumer of SanDisk USB flash drives...      0\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle\n",
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:21:16.417094Z",
     "start_time": "2021-04-28T12:21:15.956521Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 拆分資料\n",
    "df_train, df_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.1)\n",
    "\n",
    "\n",
    "# 向量化\n",
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "x_train = tfidf.fit_transform(df_train)\n",
    "x_test = tfidf.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:21:16.946124Z",
     "start_time": "2021-04-28T12:21:16.938136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 3000), (200, 3000))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:24:30.143176Z",
     "start_time": "2021-04-28T12:24:30.121406Z"
    }
   },
   "outputs": [],
   "source": [
    "## 模型套用 + 超參數調整\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_result_grid(estimator, params, model_name, Xtrain, Ytrain, Xtest, Ytest):\n",
    "    print(model_name, '結果如下:')\n",
    "    grid = GridSearchCV(estimator, param_grid=params)\n",
    "    grid.fit(Xtrain, Ytrain)\n",
    "    print(grid.best_estimator_)\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "\n",
    "    print(grid.score(Xtest, Ytest))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:21:21.173136Z",
     "start_time": "2021-04-28T12:21:19.087978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 結果如下:\n",
      "KNeighborsClassifier(n_neighbors=10)\n",
      "{'n_neighbors': 10, 'weights': 'uniform'}\n",
      "0.6888888888888889\n",
      "0.735\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "knn_param = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "}\n",
    "grid_knn = get_result_grid(KNeighborsClassifier(), knn_param, 'KNN', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:22:33.692744Z",
     "start_time": "2021-04-28T12:21:27.775594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "隨機森林 結果如下:\n",
      "RandomForestClassifier(min_samples_split=4, n_estimators=150)\n",
      "{'max_depth': None, 'min_samples_split': 4, 'n_estimators': 150}\n",
      "0.7977777777777778\n",
      "0.865\n"
     ]
    }
   ],
   "source": [
    "## 隨機森林\n",
    "\n",
    "rf_param = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 4]\n",
    "}\n",
    "\n",
    "grid_rf = get_result_grid(RandomForestClassifier(), rf_param, '隨機森林', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:30:21.119454Z",
     "start_time": "2021-04-28T12:24:38.639085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度提升樹 結果如下:\n",
      "GradientBoostingClassifier(max_depth=None)\n",
      "{'max_depth': None, 'n_estimators': 100}\n",
      "0.6699999999999999\n",
      "0.655\n"
     ]
    }
   ],
   "source": [
    "## GDBT\n",
    "\n",
    "gdbt_param = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None],\n",
    "}\n",
    "\n",
    "grid_gdbt = get_result_grid(GradientBoostingClassifier(), gdbt_param, '梯度提升樹', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:30:21.414301Z",
     "start_time": "2021-04-28T12:30:21.124456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "樸素貝式分類 結果如下:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\naive_bayes.py:509: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\naive_bayes.py:509: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\naive_bayes.py:509: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\naive_bayes.py:509: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\naive_bayes.py:509: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1)\n",
      "{'alpha': 1}\n",
      "0.8\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "## Naive bayes\n",
    "\n",
    "bayes_param = {\n",
    "    'alpha': [0, 0.5, 1],\n",
    "}\n",
    "\n",
    "grid_bayes = get_result_grid(MultinomialNB(), bayes_param, '樸素貝式分類', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:32:49.321901Z",
     "start_time": "2021-04-28T12:32:48.684403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 結果如下:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "{'max_iter': 100, 'penalty': 'l2'}\n",
      "0.8116666666666668\n",
      "0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py:921: UserWarning: One or more of the test scores are non-finite: [       nan 0.81166667        nan 0.81166667]\n",
      "  category=UserWarning\n"
     ]
    }
   ],
   "source": [
    "## Logistic\n",
    "\n",
    "log_param = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [100, 150]\n",
    "}\n",
    "\n",
    "grid_log = get_result_grid(LogisticRegression(), log_param, 'LogisticRegression', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit6893c7013b164b1189a865dcaea9fb2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
