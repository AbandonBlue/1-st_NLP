{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本日課程-文字預處理，部分內容前面章節可能提過，這裡會將前處理所需技巧串起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:21:10.759763Z",
     "start_time": "2021-01-23T15:21:03.652984Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "# tsv是指用tab分開字元的檔案\n",
    "dataset = pd.read_csv('./datasets/Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:21:12.865187Z",
     "start_time": "2021-01-23T15:21:12.854185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review before preprocessing : Wow... Loved this place.\n"
     ]
    }
   ],
   "source": [
    "print('review before preprocessing : {}'.format(dataset['Review'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 運用re.sub去除部分字元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:22:07.987619Z",
     "start_time": "2021-01-23T15:22:07.982623Z"
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "# re.sub用來去除不要字元，第一個參數是要去除字元，但可以透過添加＾，變成不要去除字元\n",
    "# 第二個參數是去除字元後這些東西要變成什麼，在這我們是希望它變成一個空格\n",
    "# 第三個參數則是我們要剝除的字元從哪裡來\n",
    "\n",
    "review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:22:12.857657Z",
     "start_time": "2021-01-23T15:22:12.851836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review after re.sub : Wow    Loved this place \n"
     ]
    }
   ],
   "source": [
    "print('review after re.sub : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將所有字母轉為小寫:因為大部分情境區分大小寫並不能提供而外訊息，如CV內顏色無法提供額外訊息時我們會將圖像轉為灰階，藉此降低複雜度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:22:31.127032Z",
     "start_time": "2021-01-23T15:22:31.122041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review after lower : wow    loved this place \n"
     ]
    }
   ],
   "source": [
    "#把全部變成小寫\n",
    "review = review.lower()\n",
    "print('review after lower : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:22:46.037507Z",
     "start_time": "2021-01-23T15:22:46.032507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review after split : ['wow', 'loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# 把 review 裡面的單字切開\n",
    "print('review after split : {}'.format(review.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tokenize 相較於split會是更好的選擇，如 split 無法分開 word. 這種case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:22:55.330764Z",
     "start_time": "2021-01-23T15:22:55.307765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow', '...', 'Loved', 'this', 'place', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('Wow... Loved this place.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:22:59.324939Z",
     "start_time": "2021-01-23T15:22:59.317932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review after tokenize : ['wow', 'loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "review = nltk.word_tokenize(review)\n",
    "print('review after tokenize : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 中文使用 jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:23:08.886250Z",
     "start_time": "2021-01-23T15:23:08.880254Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.set_dictionary('./datasets/dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:23:36.768617Z",
     "start_time": "2021-01-23T15:23:36.163070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\aband\\OneDrive\\桌面\\NLP_marathon\\NLP_practice\\1-st_NLP\\hw\\datasets\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\aband\\AppData\\Local\\Temp\\jieba.u0b2b3772ac1635337d56f7845538cf9d.cache\n",
      "Loading model cost 0.597 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: 哇|！|我|好|喜歡|這|個|地方\n"
     ]
    }
   ],
   "source": [
    "review_ = '哇！我好喜歡這個地方'\n",
    "cut_result = jieba.cut(review_, cut_all=False, HMM=False)\n",
    "print(\"output: {}\".format('|'.join(cut_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords: 移除贅字，此步驟為前處理的重要步驟之一，過多的贅字不僅無法提供更多訊息，還會干擾到模型的訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:23:45.663440Z",
     "start_time": "2021-01-23T15:23:45.125442Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aband\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#處理文字，有建立好的文字褲會幫我們移除不想要的文字\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:24:36.532404Z",
     "start_time": "2021-01-23T15:24:36.520890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 英文停用詞\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:25:10.171531Z",
     "start_time": "2021-01-23T15:25:10.157500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review after removeing stopwords : ['wow', 'loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "print('review after removeing stopwords : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stopwords.words('english') 是一個建立好的list，包含一些常見的英文贅字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:25:18.854296Z",
     "start_time": "2021-01-23T15:25:18.844872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我們也可以自己建立 stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:25:52.920654Z",
     "start_time": "2021-01-23T15:25:52.909619Z"
    }
   },
   "outputs": [],
   "source": [
    "# source:https://github.com/tomlinNTUB/Machine-Learning\n",
    "with open('./datasets/停用詞-繁體中文.txt','r', encoding='utf-8') as file:\n",
    "    stop_words = file.readlines()\n",
    "stop_words = [word.strip('\\n') for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:26:15.521970Z",
     "start_time": "2021-01-23T15:26:15.515968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "practice_sentence after removeing stopwords : ['現在', '好想', '睡覺']\n"
     ]
    }
   ],
   "source": [
    "practice_sentence = ['哈哈','!','現在','好想','睡覺','啊']\n",
    "practice_sentence=[word for word in practice_sentence if not word in set(stop_words)]\n",
    "print('practice_sentence after removeing stopwords : {}'.format(practice_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming: 詞幹提取\n",
    " * ex. loves,loved都變成love\n",
    " * 中文沒有詞幹提取的需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:26:32.395924Z",
     "start_time": "2021-01-23T15:26:32.390924Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "review=[ps.stem(word) for word in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:26:34.016923Z",
     "start_time": "2021-01-23T15:26:34.010924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review after stemming : ['wow', 'love', 'place']\n"
     ]
    }
   ],
   "source": [
    "print('review after stemming : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習清理所有的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:26:54.522635Z",
     "start_time": "2021-01-23T15:26:54.499639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset=pd.read_csv('movie_feedback.csv',encoding = 'Big5',names=['feedback', 'label'] )\n",
    "dataset = pd.read_csv('./datasets/Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:28:17.726051Z",
     "start_time": "2021-01-23T15:28:17.398005Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "row = len(dataset)\n",
    "for i in range(row):\n",
    "    review = re.sub('[^a-zA-Z]',' ',dataset['Review'][i])  # 保留英文\n",
    "    review = review.lower()  # 全小寫\n",
    "    review = review.split()  # 英文斷詞, 用nltk.word_tokensize更好\n",
    "    ps = PorterStemmer()  # 英文詞幹提取\n",
    "    ## 這裡先不用stopwords 因為 review中很多反定詞會被移掉 如isn't good, 會變成 good\n",
    "    review = [ps.stem(word) for word in review ]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手動選出現頻率較高的單字：一般來說我們不需要自己處理這個步驟，通常文字轉向量或index的api都有參數可以設定，這裡是讓大家自己練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:29:07.379653Z",
     "start_time": "2021-01-23T15:29:07.374655Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:29:07.768656Z",
     "start_time": "2021-01-23T15:29:07.642656Z"
    }
   },
   "outputs": [],
   "source": [
    "## 從整個corpus中取出所有的單詞\n",
    "whole_words = []\n",
    "for sentence in corpus:\n",
    "    for words in nltk.word_tokenize(sentence):\n",
    "        whole_words.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:29:25.089700Z",
     "start_time": "2021-01-23T15:29:25.011705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 585)\n",
      "('and', 393)\n",
      "('i', 357)\n",
      "('wa', 295)\n",
      "('a', 237)\n",
      "('to', 220)\n",
      "('is', 171)\n",
      "('it', 155)\n",
      "('thi', 143)\n",
      "('of', 127)\n",
      "('food', 127)\n",
      "('not', 118)\n",
      "('place', 112)\n",
      "('for', 110)\n",
      "('in', 109)\n",
      "('t', 97)\n",
      "('good', 95)\n",
      "('we', 88)\n",
      "('servic', 87)\n",
      "('be', 81)\n",
      "('veri', 76)\n",
      "('my', 73)\n",
      "('with', 72)\n",
      "('great', 70)\n",
      "('that', 70)\n",
      "('had', 70)\n",
      "('so', 66)\n",
      "('you', 66)\n",
      "('were', 63)\n",
      "('are', 63)\n",
      "('have', 63)\n",
      "('go', 62)\n",
      "('but', 62)\n",
      "('back', 61)\n",
      "('they', 58)\n",
      "('here', 57)\n",
      "('on', 55)\n",
      "('time', 55)\n",
      "('at', 55)\n",
      "('like', 51)\n",
      "('all', 44)\n",
      "('s', 42)\n",
      "('our', 41)\n",
      "('will', 37)\n",
      "('there', 36)\n",
      "('as', 36)\n",
      "('realli', 36)\n",
      "('just', 35)\n",
      "('love', 33)\n",
      "('an', 32)\n",
      "('their', 31)\n",
      "('if', 30)\n",
      "('disappoint', 30)\n",
      "('best', 30)\n",
      "('would', 29)\n",
      "('wait', 29)\n",
      "('get', 28)\n",
      "('ever', 28)\n",
      "('restaur', 28)\n",
      "('order', 28)\n",
      "('also', 27)\n",
      "('friendli', 27)\n",
      "('eat', 27)\n",
      "('one', 27)\n",
      "('up', 26)\n",
      "('onli', 26)\n",
      "('never', 26)\n",
      "('don', 26)\n",
      "('can', 26)\n",
      "('no', 25)\n",
      "('your', 25)\n",
      "('out', 25)\n",
      "('nice', 25)\n",
      "('been', 24)\n",
      "('what', 24)\n",
      "('amaz', 24)\n",
      "('again', 24)\n",
      "('delici', 24)\n",
      "('from', 24)\n",
      "('ve', 23)\n",
      "('price', 22)\n",
      "('tast', 22)\n",
      "('vega', 22)\n",
      "('did', 22)\n",
      "('by', 21)\n",
      "('which', 21)\n",
      "('come', 21)\n",
      "('m', 21)\n",
      "('even', 21)\n",
      "('pretti', 20)\n",
      "('some', 20)\n",
      "('me', 20)\n",
      "('came', 20)\n",
      "('when', 20)\n",
      "('experi', 20)\n",
      "('staff', 19)\n",
      "('he', 19)\n",
      "('us', 19)\n",
      "('minut', 19)\n",
      "('definit', 19)\n",
      "('or', 19)\n",
      "('won', 19)\n",
      "('recommend', 18)\n",
      "('say', 18)\n",
      "('chicken', 18)\n",
      "('server', 18)\n",
      "('bad', 18)\n",
      "('much', 18)\n",
      "('star', 18)\n",
      "('got', 18)\n",
      "('steak', 18)\n",
      "('flavor', 18)\n",
      "('pizza', 18)\n",
      "('want', 17)\n",
      "('made', 17)\n",
      "('tri', 17)\n",
      "('salad', 17)\n",
      "('first', 17)\n",
      "('think', 17)\n",
      "('more', 17)\n",
      "('dish', 17)\n",
      "('about', 17)\n",
      "('menu', 16)\n",
      "('could', 16)\n",
      "('fri', 16)\n",
      "('ha', 16)\n",
      "('burger', 16)\n",
      "('way', 16)\n",
      "('other', 16)\n",
      "('better', 16)\n",
      "('too', 15)\n",
      "('worst', 15)\n",
      "('feel', 15)\n",
      "('alway', 15)\n",
      "('than', 15)\n",
      "('well', 15)\n",
      "('how', 15)\n",
      "('fresh', 14)\n",
      "('becaus', 14)\n",
      "('tabl', 14)\n",
      "('do', 14)\n",
      "('qualiti', 14)\n",
      "('meal', 14)\n",
      "('them', 14)\n",
      "('didn', 13)\n",
      "('thing', 13)\n",
      "('went', 13)\n",
      "('ll', 13)\n",
      "('sushi', 13)\n",
      "('wasn', 13)\n",
      "('select', 12)\n",
      "('now', 12)\n",
      "('enough', 12)\n",
      "('ani', 12)\n",
      "('after', 12)\n",
      "('enjoy', 12)\n",
      "('over', 12)\n",
      "('buffet', 12)\n",
      "('everyth', 12)\n",
      "('serv', 12)\n",
      "('fantast', 12)\n",
      "('night', 12)\n",
      "('perfect', 12)\n",
      "('awesom', 12)\n",
      "('tasti', 11)\n",
      "('still', 11)\n",
      "('slow', 11)\n",
      "('down', 11)\n",
      "('sauc', 11)\n",
      "('bland', 11)\n",
      "('next', 11)\n",
      "('atmospher', 11)\n",
      "('know', 11)\n",
      "('manag', 11)\n",
      "('littl', 10)\n",
      "('worth', 10)\n",
      "('ask', 10)\n",
      "('side', 10)\n",
      "('cook', 10)\n",
      "('sandwich', 10)\n",
      "('give', 10)\n",
      "('lunch', 10)\n",
      "('meat', 10)\n",
      "('dine', 10)\n",
      "('breakfast', 10)\n",
      "('anoth', 10)\n",
      "('check', 10)\n",
      "('excel', 10)\n",
      "('probabl', 10)\n",
      "('terribl', 10)\n",
      "('off', 9)\n",
      "('potato', 9)\n",
      "('waitress', 9)\n",
      "('right', 9)\n",
      "('beer', 9)\n",
      "('deal', 9)\n",
      "('look', 9)\n",
      "('lot', 9)\n",
      "('everi', 9)\n",
      "('suck', 9)\n",
      "('peopl', 9)\n",
      "('custom', 9)\n",
      "('absolut', 9)\n",
      "('befor', 9)\n",
      "('d', 9)\n",
      "('drink', 9)\n",
      "('expect', 9)\n",
      "('quit', 9)\n",
      "('bit', 9)\n",
      "('spot', 9)\n",
      "('reason', 9)\n",
      "('rude', 9)\n",
      "('bar', 9)\n",
      "('mani', 9)\n",
      "('clean', 9)\n",
      "('day', 9)\n",
      "('seat', 9)\n",
      "('impress', 9)\n",
      "('she', 9)\n",
      "('hot', 9)\n",
      "('soon', 9)\n",
      "('friend', 9)\n",
      "('need', 9)\n",
      "('cold', 9)\n",
      "('make', 9)\n",
      "('then', 9)\n",
      "('sure', 8)\n",
      "('took', 8)\n",
      "('hour', 8)\n",
      "('around', 8)\n",
      "('overal', 8)\n",
      "('attent', 8)\n",
      "('dessert', 8)\n",
      "('wast', 8)\n",
      "('both', 8)\n",
      "('busi', 8)\n",
      "('spici', 8)\n",
      "('who', 8)\n",
      "('said', 8)\n",
      "('review', 8)\n",
      "('sinc', 8)\n",
      "('while', 8)\n",
      "('take', 8)\n",
      "('chip', 8)\n",
      "('felt', 8)\n",
      "('special', 8)\n",
      "('owner', 8)\n",
      "('few', 8)\n",
      "('two', 8)\n",
      "('happi', 8)\n",
      "('overpr', 7)\n",
      "('warm', 7)\n",
      "('found', 7)\n",
      "('portion', 7)\n",
      "('waiter', 7)\n",
      "('tender', 7)\n",
      "('insid', 7)\n",
      "('hi', 7)\n",
      "('wonder', 7)\n",
      "('noth', 7)\n",
      "('help', 7)\n",
      "('town', 7)\n",
      "('lack', 7)\n",
      "('ambianc', 7)\n",
      "('return', 7)\n",
      "('serious', 7)\n",
      "('must', 7)\n",
      "('area', 7)\n",
      "('onc', 7)\n",
      "('stay', 7)\n",
      "('far', 7)\n",
      "('dinner', 7)\n",
      "('famili', 7)\n",
      "('super', 7)\n",
      "('am', 6)\n",
      "('pho', 6)\n",
      "('tell', 6)\n",
      "('taco', 6)\n",
      "('total', 6)\n",
      "('poor', 6)\n",
      "('shrimp', 6)\n",
      "('hard', 6)\n",
      "('rare', 6)\n",
      "('left', 6)\n",
      "('horribl', 6)\n",
      "('talk', 6)\n",
      "('each', 6)\n",
      "('pasta', 6)\n",
      "('should', 6)\n",
      "('where', 6)\n",
      "('re', 6)\n",
      "('hit', 6)\n",
      "('mediocr', 6)\n",
      "('done', 6)\n",
      "('avoid', 6)\n",
      "('hand', 6)\n",
      "('ice', 6)\n",
      "('live', 6)\n",
      "('hope', 6)\n",
      "('last', 6)\n",
      "('her', 6)\n",
      "('small', 6)\n",
      "('see', 6)\n",
      "('either', 6)\n",
      "('money', 6)\n",
      "('twice', 6)\n",
      "('pleas', 6)\n",
      "('authent', 6)\n",
      "('bring', 6)\n",
      "('bare', 6)\n",
      "('stop', 5)\n",
      "('dure', 5)\n",
      "('may', 5)\n",
      "('kept', 5)\n",
      "('beauti', 5)\n",
      "('quick', 5)\n",
      "('visit', 5)\n",
      "('delight', 5)\n",
      "('these', 5)\n",
      "('sick', 5)\n",
      "('beef', 5)\n",
      "('outsid', 5)\n",
      "('huge', 5)\n",
      "('heart', 5)\n",
      "('seafood', 5)\n",
      "('tasteless', 5)\n",
      "('fish', 5)\n",
      "('use', 5)\n",
      "('sweet', 5)\n",
      "('old', 5)\n",
      "('year', 5)\n",
      "('roll', 5)\n",
      "('larg', 5)\n",
      "('mouth', 5)\n",
      "('arriv', 5)\n",
      "('wine', 5)\n",
      "('trip', 5)\n",
      "('real', 5)\n",
      "('thai', 5)\n",
      "('eaten', 5)\n",
      "('thought', 5)\n",
      "('away', 5)\n",
      "('leav', 5)\n",
      "('extrem', 5)\n",
      "('bathroom', 5)\n",
      "('decor', 5)\n",
      "('consid', 5)\n",
      "('watch', 5)\n",
      "('pay', 5)\n",
      "('full', 5)\n",
      "('treat', 5)\n",
      "('bacon', 5)\n",
      "('zero', 5)\n",
      "('sat', 5)\n",
      "('incred', 5)\n",
      "('bread', 5)\n",
      "('home', 5)\n",
      "('egg', 5)\n",
      "('drive', 5)\n",
      "('chef', 5)\n",
      "('close', 5)\n",
      "('set', 5)\n",
      "('fast', 5)\n",
      "('wrong', 5)\n",
      "('locat', 5)\n",
      "('soup', 5)\n",
      "('anytim', 5)\n",
      "('new', 5)\n",
      "('long', 5)\n",
      "('dri', 5)\n",
      "('damn', 4)\n",
      "('care', 4)\n",
      "('end', 4)\n",
      "('disgust', 4)\n",
      "('highli', 4)\n",
      "('let', 4)\n",
      "('wall', 4)\n",
      "('sever', 4)\n",
      "('run', 4)\n",
      "('salmon', 4)\n",
      "('seem', 4)\n",
      "('bite', 4)\n",
      "('elsewher', 4)\n",
      "('establish', 4)\n",
      "('gross', 4)\n",
      "('melt', 4)\n",
      "('gener', 4)\n",
      "('dress', 4)\n",
      "('husband', 4)\n",
      "('imagin', 4)\n",
      "('grill', 4)\n",
      "('least', 4)\n",
      "('water', 4)\n",
      "('receiv', 4)\n",
      "('rice', 4)\n",
      "('season', 4)\n",
      "('today', 4)\n",
      "('parti', 4)\n",
      "('person', 4)\n",
      "('roast', 4)\n",
      "('everyon', 4)\n",
      "('yummi', 4)\n",
      "('ladi', 4)\n",
      "('beat', 4)\n",
      "('kind', 4)\n",
      "('prepar', 4)\n",
      "('pork', 4)\n",
      "('cream', 4)\n",
      "('fact', 4)\n",
      "('itself', 4)\n",
      "('thumb', 4)\n",
      "('bay', 4)\n",
      "('differ', 4)\n",
      "('piec', 4)\n",
      "('unfortun', 4)\n",
      "('phoenix', 4)\n",
      "('sad', 4)\n",
      "('list', 4)\n",
      "('someth', 4)\n",
      "('wing', 4)\n",
      "('insult', 4)\n",
      "('though', 4)\n",
      "('someon', 4)\n",
      "('folk', 4)\n",
      "('fun', 4)\n",
      "('coupl', 4)\n",
      "('possibl', 4)\n",
      "('work', 4)\n",
      "('job', 4)\n",
      "('tea', 4)\n",
      "('whi', 4)\n",
      "('gave', 4)\n",
      "('rate', 4)\n",
      "('high', 4)\n",
      "('hous', 4)\n",
      "('dirti', 4)\n",
      "('veget', 4)\n",
      "('green', 4)\n",
      "('bean', 4)\n",
      "('option', 4)\n",
      "('thin', 4)\n",
      "('recent', 4)\n",
      "('averag', 4)\n",
      "('wow', 3)\n",
      "('textur', 3)\n",
      "('nasti', 3)\n",
      "('under', 3)\n",
      "('touch', 3)\n",
      "('brought', 3)\n",
      "('sashimi', 3)\n",
      "('final', 3)\n",
      "('yourself', 3)\n",
      "('moist', 3)\n",
      "('judg', 3)\n",
      "('provid', 3)\n",
      "('frozen', 3)\n",
      "('greek', 3)\n",
      "('pita', 3)\n",
      "('hummu', 3)\n",
      "('duck', 3)\n",
      "('flat', 3)\n",
      "('amount', 3)\n",
      "('perfectli', 3)\n",
      "('appet', 3)\n",
      "('second', 3)\n",
      "('salt', 3)\n",
      "('chewi', 3)\n",
      "('min', 3)\n",
      "('guess', 3)\n",
      "('break', 3)\n",
      "('walk', 3)\n",
      "('oh', 3)\n",
      "('step', 3)\n",
      "('tip', 3)\n",
      "('quickli', 3)\n",
      "('cafe', 3)\n",
      "('wife', 3)\n",
      "('bartend', 3)\n",
      "('ambienc', 3)\n",
      "('music', 3)\n",
      "('play', 3)\n",
      "('lover', 3)\n",
      "('honest', 3)\n",
      "('although', 3)\n",
      "('actual', 3)\n",
      "('fine', 3)\n",
      "('guy', 3)\n",
      "('word', 3)\n",
      "('wouldn', 3)\n",
      "('strip', 3)\n",
      "('belli', 3)\n",
      "('wrap', 3)\n",
      "('delish', 3)\n",
      "('chees', 3)\n",
      "('subway', 3)\n",
      "('empti', 3)\n",
      "('ate', 3)\n",
      "('top', 3)\n",
      "('cover', 3)\n",
      "('stale', 3)\n",
      "('highlight', 3)\n",
      "('ago', 3)\n",
      "('immedi', 3)\n",
      "('liter', 3)\n",
      "('miss', 3)\n",
      "('vegetarian', 3)\n",
      "('decid', 3)\n",
      "('healthi', 3)\n",
      "('interest', 3)\n",
      "('butter', 3)\n",
      "('gyro', 3)\n",
      "('satisfi', 3)\n",
      "('dog', 3)\n",
      "('brunch', 3)\n",
      "('white', 3)\n",
      "('soggi', 3)\n",
      "('unless', 3)\n",
      "('lobster', 3)\n",
      "('cours', 3)\n",
      "('none', 3)\n",
      "('main', 3)\n",
      "('isn', 3)\n",
      "('world', 3)\n",
      "('yet', 3)\n",
      "('couldn', 3)\n",
      "('point', 3)\n",
      "('par', 3)\n",
      "('patio', 3)\n",
      "('outstand', 3)\n",
      "('hate', 3)\n",
      "('pull', 3)\n",
      "('toast', 3)\n",
      "('boy', 3)\n",
      "('favorit', 3)\n",
      "('especi', 3)\n",
      "('mom', 3)\n",
      "('pleasant', 3)\n",
      "('equal', 3)\n",
      "('pancak', 3)\n",
      "('door', 3)\n",
      "('offer', 3)\n",
      "('cool', 3)\n",
      "('els', 3)\n",
      "('find', 3)\n",
      "('deserv', 3)\n",
      "('stomach', 3)\n",
      "('told', 3)\n",
      "('shop', 3)\n",
      "('kid', 3)\n",
      "('complet', 3)\n",
      "('regular', 3)\n",
      "('until', 3)\n",
      "('biscuit', 3)\n",
      "('anyway', 3)\n",
      "('big', 3)\n",
      "('contain', 3)\n",
      "('group', 3)\n",
      "('rather', 3)\n",
      "('bill', 3)\n",
      "('slice', 3)\n",
      "('wors', 3)\n",
      "('fill', 3)\n",
      "('homemad', 3)\n",
      "('such', 3)\n",
      "('gone', 3)\n",
      "('those', 3)\n",
      "('boyfriend', 3)\n",
      "('room', 3)\n",
      "('mayb', 3)\n",
      "('believ', 3)\n",
      "('sit', 3)\n",
      "('paper', 3)\n",
      "('ok', 3)\n",
      "('mean', 3)\n",
      "('half', 3)\n",
      "('vibe', 3)\n",
      "('him', 3)\n",
      "('edibl', 3)\n",
      "('fail', 3)\n",
      "('spend', 3)\n",
      "('crowd', 3)\n",
      "('style', 3)\n",
      "('salsa', 3)\n",
      "('aw', 3)\n",
      "('crust', 2)\n",
      "('late', 2)\n",
      "('cashier', 2)\n",
      "('mmmm', 2)\n",
      "('human', 2)\n",
      "('hair', 2)\n",
      "('sign', 2)\n",
      "('cute', 2)\n",
      "('red', 2)\n",
      "('cake', 2)\n",
      "('stuff', 2)\n",
      "('mexican', 2)\n",
      "('overwhelm', 2)\n",
      "('combo', 2)\n",
      "('decent', 2)\n",
      "('blow', 2)\n",
      "('familiar', 2)\n",
      "('favor', 2)\n",
      "('inexpens', 2)\n",
      "('into', 2)\n",
      "('note', 2)\n",
      "('behind', 2)\n",
      "('char', 2)\n",
      "('realiz', 2)\n",
      "('attitud', 2)\n",
      "('toward', 2)\n",
      "('attack', 2)\n",
      "('downtown', 2)\n",
      "('excus', 2)\n",
      "('scallop', 2)\n",
      "('rip', 2)\n",
      "('refil', 2)\n",
      "('cocktail', 2)\n",
      "('glad', 2)\n",
      "('heard', 2)\n",
      "('batter', 2)\n",
      "('finish', 2)\n",
      "('includ', 2)\n",
      "('abov', 2)\n",
      "('beyond', 2)\n",
      "('meh', 2)\n",
      "('known', 2)\n",
      "('valu', 2)\n",
      "('opportun', 2)\n",
      "('compani', 2)\n",
      "('experienc', 2)\n",
      "('underwhelm', 2)\n",
      "('smell', 2)\n",
      "('greas', 2)\n",
      "('rave', 2)\n",
      "('sugari', 2)\n",
      "('six', 2)\n",
      "('die', 2)\n",
      "('bye', 2)\n",
      "('garlic', 2)\n",
      "('marrow', 2)\n",
      "('ad', 2)\n",
      "('extra', 2)\n",
      "('mari', 2)\n",
      "('pass', 2)\n",
      "('between', 2)\n",
      "('indian', 2)\n",
      "('boot', 2)\n",
      "('ventur', 2)\n",
      "('crispi', 2)\n",
      "('tuna', 2)\n",
      "('bagel', 2)\n",
      "('suggest', 2)\n",
      "('curri', 2)\n",
      "('pace', 2)\n",
      "('ignor', 2)\n",
      "('greet', 2)\n",
      "('joint', 2)\n",
      "('cut', 2)\n",
      "('bakeri', 2)\n",
      "('hip', 2)\n",
      "('overcook', 2)\n",
      "('charcoal', 2)\n",
      "('dirt', 2)\n",
      "('station', 2)\n",
      "('valley', 2)\n",
      "('bowl', 2)\n",
      "('disrespect', 2)\n",
      "('mushroom', 2)\n",
      "('gold', 2)\n",
      "('pure', 2)\n",
      "('bug', 2)\n",
      "('show', 2)\n",
      "('given', 2)\n",
      "('tartar', 2)\n",
      "('shower', 2)\n",
      "('mind', 2)\n",
      "('bisqu', 2)\n",
      "('filet', 2)\n",
      "('pepper', 2)\n",
      "('date', 2)\n",
      "('unbeliev', 2)\n",
      "('doubl', 2)\n",
      "('cheeseburg', 2)\n",
      "('singl', 2)\n",
      "('pictur', 2)\n",
      "('event', 2)\n",
      "('yum', 2)\n",
      "('mayo', 2)\n",
      "('honestli', 2)\n",
      "('eye', 2)\n",
      "('almost', 2)\n",
      "('build', 2)\n",
      "('light', 2)\n",
      "('dark', 2)\n",
      "('mood', 2)\n",
      "('sub', 2)\n",
      "('creami', 2)\n",
      "('similar', 2)\n",
      "('complaint', 2)\n",
      "('peanut', 2)\n",
      "('stick', 2)\n",
      "('charg', 2)\n",
      "('tap', 2)\n",
      "('plu', 2)\n",
      "('buck', 2)\n",
      "('coffe', 2)\n",
      "('cant', 2)\n",
      "('boba', 2)\n",
      "('bachi', 2)\n",
      "('later', 2)\n",
      "('neighborhood', 2)\n",
      "('conveni', 2)\n",
      "('soooo', 2)\n",
      "('third', 2)\n",
      "('stir', 2)\n",
      "('box', 2)\n",
      "('summer', 2)\n",
      "('basic', 2)\n",
      "('joke', 2)\n",
      "('fare', 2)\n",
      "('without', 2)\n",
      "('doubt', 2)\n",
      "('black', 2)\n",
      "('seen', 2)\n",
      "('omg', 2)\n",
      "('brick', 2)\n",
      "('oven', 2)\n",
      "('multipl', 2)\n",
      "('ten', 2)\n",
      "('life', 2)\n",
      "('handl', 2)\n",
      "('lukewarm', 2)\n",
      "('comfort', 2)\n",
      "('eggplant', 2)\n",
      "('part', 2)\n",
      "('happen', 2)\n",
      "('car', 2)\n",
      "('front', 2)\n",
      "('disgrac', 2)\n",
      "('anyon', 2)\n",
      "('stuf', 2)\n",
      "('mall', 2)\n",
      "('impecc', 2)\n",
      "('simpli', 2)\n",
      "('remind', 2)\n",
      "('pop', 2)\n",
      "('assur', 2)\n",
      "('sore', 2)\n",
      "('becom', 2)\n",
      "('profession', 2)\n",
      "('mistak', 2)\n",
      "('nicest', 2)\n",
      "('cow', 2)\n",
      "('week', 2)\n",
      "('combin', 2)\n",
      "('driest', 2)\n",
      "('relax', 2)\n",
      "('tot', 2)\n",
      "('paid', 2)\n",
      "('acknowledg', 2)\n",
      "('margarita', 2)\n",
      "('flower', 2)\n",
      "('crab', 2)\n",
      "('leg', 2)\n",
      "('dont', 2)\n",
      "('worker', 2)\n",
      "('bunch', 2)\n",
      "('call', 2)\n",
      "('apolog', 2)\n",
      "('pack', 2)\n",
      "('whole', 2)\n",
      "('choos', 2)\n",
      "('entre', 2)\n",
      "('howev', 2)\n",
      "('tapa', 2)\n",
      "('vinegrett', 2)\n",
      "('babi', 2)\n",
      "('guest', 2)\n",
      "('fan', 2)\n",
      "('cheap', 2)\n",
      "('present', 2)\n",
      "('color', 2)\n",
      "('fairli', 2)\n",
      "('non', 2)\n",
      "('focus', 2)\n",
      "('same', 2)\n",
      "('promis', 2)\n",
      "('italian', 2)\n",
      "('legit', 2)\n",
      "('plate', 2)\n",
      "('needless', 2)\n",
      "('longer', 2)\n",
      "('read', 2)\n",
      "('simpl', 2)\n",
      "('classic', 2)\n",
      "('fli', 2)\n",
      "('chines', 2)\n",
      "('anyth', 2)\n",
      "('complain', 2)\n",
      "('spice', 2)\n",
      "('mid', 2)\n",
      "('defin', 2)\n",
      "('low', 2)\n",
      "('bother', 2)\n",
      "('flavorless', 2)\n",
      "('nacho', 2)\n",
      "('doe', 2)\n",
      "('crazi', 2)\n",
      "('tribut', 2)\n",
      "('surpris', 2)\n",
      "('fell', 2)\n",
      "('employe', 2)\n",
      "('put', 2)\n",
      "('head', 2)\n",
      "('heat', 2)\n",
      "('start', 2)\n",
      "('aren', 2)\n",
      "('three', 2)\n",
      "('correct', 2)\n",
      "('lost', 2)\n",
      "('undercook', 2)\n",
      "('bank', 1)\n",
      "('holiday', 1)\n",
      "('rick', 1)\n",
      "('steve', 1)\n",
      "('angri', 1)\n",
      "('honeslti', 1)\n",
      "('rubber', 1)\n",
      "('ahead', 1)\n",
      "('warmer', 1)\n",
      "('prompt', 1)\n",
      "('wayyy', 1)\n",
      "('cape', 1)\n",
      "('cod', 1)\n",
      "('ravoli', 1)\n",
      "('cranberri', 1)\n",
      "('shock', 1)\n",
      "('indic', 1)\n",
      "('cash', 1)\n",
      "('alon', 1)\n",
      "('burritto', 1)\n",
      "('blah', 1)\n",
      "('less', 1)\n",
      "('interior', 1)\n",
      "('perform', 1)\n",
      "('velvet', 1)\n",
      "('ohhh', 1)\n",
      "('hole', 1)\n",
      "('street', 1)\n",
      "('luke', 1)\n",
      "('accid', 1)\n",
      "('happier', 1)\n",
      "('grab', 1)\n",
      "('pub', 1)\n",
      "('redeem', 1)\n",
      "('ampl', 1)\n",
      "('stupid', 1)\n",
      "('hiro', 1)\n",
      "('drag', 1)\n",
      "('whether', 1)\n",
      "('styrofoam', 1)\n",
      "('fear', 1)\n",
      "('posit', 1)\n",
      "('puck', 1)\n",
      "('regist', 1)\n",
      "('prime', 1)\n",
      "('rib', 1)\n",
      "('section', 1)\n",
      "('firehous', 1)\n",
      "('refresh', 1)\n",
      "('pink', 1)\n",
      "('sunglass', 1)\n",
      "('chow', 1)\n",
      "('mein', 1)\n",
      "('line', 1)\n",
      "('string', 1)\n",
      "('bottom', 1)\n",
      "('power', 1)\n",
      "('banana', 1)\n",
      "('petrifi', 1)\n",
      "('struggl', 1)\n",
      "('wave', 1)\n",
      "('handmad', 1)\n",
      "('militari', 1)\n",
      "('discount', 1)\n",
      "('gringo', 1)\n",
      "('updat', 1)\n",
      "('appar', 1)\n",
      "('jeff', 1)\n",
      "('milkshak', 1)\n",
      "('chocol', 1)\n",
      "('milk', 1)\n",
      "('excalibur', 1)\n",
      "('common', 1)\n",
      "('sens', 1)\n",
      "('appal', 1)\n",
      "('cheat', 1)\n",
      "('relationship', 1)\n",
      "('trap', 1)\n",
      "('turkey', 1)\n",
      "('pan', 1)\n",
      "('disast', 1)\n",
      "('tailor', 1)\n",
      "('palat', 1)\n",
      "('spring', 1)\n",
      "('ratio', 1)\n",
      "('unsatisfi', 1)\n",
      "('omelet', 1)\n",
      "('summari', 1)\n",
      "('sexi', 1)\n",
      "('outrag', 1)\n",
      "('flirt', 1)\n",
      "('hottest', 1)\n",
      "('rock', 1)\n",
      "('casino', 1)\n",
      "('forward', 1)\n",
      "('bone', 1)\n",
      "('bloddi', 1)\n",
      "('mussel', 1)\n",
      "('reduct', 1)\n",
      "('tigerlilli', 1)\n",
      "('afternoon', 1)\n",
      "('sooooo', 1)\n",
      "('yama', 1)\n",
      "('mess', 1)\n",
      "('sound', 1)\n",
      "('blandest', 1)\n",
      "('cuisin', 1)\n",
      "('worri', 1)\n",
      "('son', 1)\n",
      "('further', 1)\n",
      "('host', 1)\n",
      "('bitch', 1)\n",
      "('number', 1)\n",
      "('phenomen', 1)\n",
      "('penn', 1)\n",
      "('vodka', 1)\n",
      "('massiv', 1)\n",
      "('meatloaf', 1)\n",
      "('nyc', 1)\n",
      "('lox', 1)\n",
      "('caper', 1)\n",
      "('meet', 1)\n",
      "('solid', 1)\n",
      "('weekend', 1)\n",
      "('bamboo', 1)\n",
      "('shoot', 1)\n",
      "('blanket', 1)\n",
      "('moz', 1)\n",
      "('subpar', 1)\n",
      "('chang', 1)\n",
      "('fianc', 1)\n",
      "('middl', 1)\n",
      "('mandalay', 1)\n",
      "('forti', 1)\n",
      "('five', 1)\n",
      "('vain', 1)\n",
      "('crostini', 1)\n",
      "('nigiri', 1)\n",
      "('voodoo', 1)\n",
      "('gluten', 1)\n",
      "('free', 1)\n",
      "('leftov', 1)\n",
      "('reloc', 1)\n",
      "('divers', 1)\n",
      "('cost', 1)\n",
      "('metro', 1)\n",
      "('hella', 1)\n",
      "('salti', 1)\n",
      "('spinach', 1)\n",
      "('avocado', 1)\n",
      "('ingredi', 1)\n",
      "('lordi', 1)\n",
      "('khao', 1)\n",
      "('soi', 1)\n",
      "('terrif', 1)\n",
      "('thrill', 1)\n",
      "('accommod', 1)\n",
      "('daughter', 1)\n",
      "('perhap', 1)\n",
      "('caught', 1)\n",
      "('inspir', 1)\n",
      "('desir', 1)\n",
      "('modern', 1)\n",
      "('maintain', 1)\n",
      "('cozi', 1)\n",
      "('weekli', 1)\n",
      "('haunt', 1)\n",
      "('send', 1)\n",
      "('verg', 1)\n",
      "('quantiti', 1)\n",
      "('lemon', 1)\n",
      "('raspberri', 1)\n",
      "('crepe', 1)\n",
      "('origin', 1)\n",
      "('joey', 1)\n",
      "('vote', 1)\n",
      "('reader', 1)\n",
      "('magazin', 1)\n",
      "('friday', 1)\n"
     ]
    }
   ],
   "source": [
    "## 取出出現頻率top_k的單詞\n",
    "top_k = 1000\n",
    "top_k_words = []\n",
    "# 前1000的字詞\n",
    "for item in Counter(whole_words).most_common(top_k):\n",
    "    print(item)\n",
    "    top_k_words.append(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以 corpus中第一個句子為範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:30:24.159902Z",
     "start_time": "2021-01-23T15:30:24.152909Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_low_frequency_word = ' '.join([word for word in nltk.word_tokenize(corpus[0]) if word in set(top_k_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:30:26.665800Z",
     "start_time": "2021-01-23T15:30:26.659763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing low frequency words:\n",
      " wow love thi place\n",
      "\n",
      "\n",
      "After removing low frequency words:\n",
      " wow love thi place\n"
     ]
    }
   ],
   "source": [
    "print('Before removing low frequency words:\\n {}'.format(corpus[0]))\n",
    "print('\\n')\n",
    "print('After removing low frequency words:\\n {}'.format(remove_low_frequency_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 轉bag-of-words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:31:00.946257Z",
     "start_time": "2021-01-23T15:31:00.918259Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Creating bag of word model\n",
    "#tokenization(符號化)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#max_features是要建造幾個column，會按造字出現的高低去篩選 \n",
    "cv = CountVectorizer(max_features=1000)\n",
    "#toarray是建造matrixs\n",
    "#X現在為sparsity就是很多零的matrix\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 選擇練習: 將處理好數據放入 naive_bayes模型，並預測評論為正向或負面，詳細原理之後章節會解釋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:31:11.603175Z",
     "start_time": "2021-01-23T15:31:11.558175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:31:19.366702Z",
     "start_time": "2021-01-23T15:31:19.358705Z"
    }
   },
   "outputs": [],
   "source": [
    "message='I really like this!!'\n",
    "## 要使用一樣的前處理\n",
    "review=re.sub('[^a-zA-Z]',' ',message)\n",
    "review=review.lower()\n",
    "review=review.split()\n",
    "ps=PorterStemmer()\n",
    "review=[ps.stem(word) for word in review]\n",
    "review = ' '.join(review)\n",
    "input_ = cv.transform([review]).toarray()\n",
    "prediction = classifier.predict(input_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:31:20.892749Z",
     "start_time": "2021-01-23T15:31:20.885709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction ## 1代表正向評價"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:31:23.573705Z",
     "start_time": "2021-01-23T15:31:23.566706Z"
    }
   },
   "outputs": [],
   "source": [
    "message='All dishes are disgusting !!'\n",
    "review=re.sub('[^a-zA-Z]',' ',message)\n",
    "review=review.lower()\n",
    "review=review.split()\n",
    "ps=PorterStemmer()\n",
    "review=[ps.stem(word) for word in review]\n",
    "review = ' '.join(review)\n",
    "input_ = cv.transform([review]).toarray()\n",
    "prediction = classifier.predict(input_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T15:31:24.418806Z",
     "start_time": "2021-01-23T15:31:24.412804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction ## 0代表負面評價"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
